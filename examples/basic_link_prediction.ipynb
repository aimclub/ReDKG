{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Install packages\n",
    "!pip install torch==1.12.0+cpu matplotlib==3.8.2 torch-geometric==2.4.0 --extra-index-url https://download.pytorch.org/whl/cpu"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b6bf145781d8f1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Download large graph dataset\n",
    "!wget https://raw.githubusercontent.com/ZhongTr0n/JD_Analysis/main/jd_data2.json "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2c1baa9b415d20d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from redkg.models.gcn import GCN\n",
    "from redkg.models.gat import GAT\n",
    "from redkg.models.graphsage import GraphSAGE\n",
    "import numpy as np\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ],
   "metadata": {
    "collapsed": true
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load dataset from file\n",
    "with open('jd_data2.json', 'r') as f:\n",
    "    graph_data = json.load(f)\n",
    "\n",
    "# Extract list of nodes and convert it to a dictionary for fast search\n",
    "node_list = [node['id'] for node in graph_data['nodes']]\n",
    "node_mapping = {node_id: i for i, node_id in enumerate(node_list)}\n",
    "node_index = {index: node for node, index in node_mapping.items()}\n",
    "\n",
    "# Create list of edges in PyTorch Geometric format\n",
    "edge_index = [[node_mapping[link['source']], node_mapping[link['target']]] for link in graph_data['links']]\n",
    "edge_weights = [link['value'] for link in graph_data['links']]\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "features = torch.randn(len(node_list), 1)\n",
    "labels = torch.tensor(list(range(len(graph_data['nodes']))), dtype=torch.long)\n",
    "\n",
    "large_dataset = Data(x=features, edge_index=edge_index, y=labels, node_mapping=node_mapping, node_index=node_index)\n",
    "#torch.save(large_dataset, 'large_dataset.pth')\n",
    "#large_dataset.cuda()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "888755f2251f5649"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "from redkg.visualization.contracts.graph_contract import GraphContract\n",
    "from redkg.visualization.contracts.graph_visualization_contract import GraphVisualizationContract\n",
    "from redkg.visualization.graph_visualization import GraphVisualizer\n",
    "\n",
    "edge_list = list(map(tuple, edge_index.t().tolist()))\n",
    "\n",
    "graph_contract: GraphContract = GraphContract(\n",
    "    vertex_num=len(graph_data['nodes']),\n",
    "    edge_list=(edge_list, edge_weights),\n",
    "    edge_num=len(graph_data['links']),\n",
    "    edge_weights=list(tensor(edge_weights * 2)),\n",
    ")\n",
    "\n",
    "vis_contract: GraphVisualizationContract = GraphVisualizationContract(\n",
    "    graph=graph_contract,\n",
    "    font_size=4.0,\n",
    "    vertex_label=node_list\n",
    ")\n",
    "\n",
    "vis: GraphVisualizer = GraphVisualizer(vis_contract)\n",
    "fig = vis.draw()\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "207f47a0e2e5e3fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generate multiple subgaphs\n",
    "def generate_subgraphs(dataset, num_subgraphs=5, min_nodes=2, max_nodes=5):\n",
    "    subgraphs = []\n",
    "    for _ in range(num_subgraphs):\n",
    "        selected_nodes = []\n",
    "        while len(selected_nodes) < random.randint(min_nodes, max_nodes):\n",
    "            if selected_nodes:\n",
    "                new_node = random.choice(\n",
    "                    [link['target'] for link in dataset['links'] if\n",
    "                     link['source'] in {node['id'] for node in selected_nodes}] +\n",
    "                    [link['source'] for link in dataset['links'] if\n",
    "                     link['target'] in {node['id'] for node in selected_nodes}]\n",
    "                )\n",
    "            else:\n",
    "                new_node = random.choice(dataset['nodes'])['id']\n",
    "            if new_node not in {node['id'] for node in selected_nodes}:\n",
    "                selected_nodes.append({'id': new_node})\n",
    "        selected_node_ids = {node['id'] for node in selected_nodes}\n",
    "        selected_links = [link for link in dataset['links'] if\n",
    "                          link['source'] in selected_node_ids and link['target'] in selected_node_ids]\n",
    "        subgraphs.append({'nodes': selected_nodes, 'links': selected_links})\n",
    "    return subgraphs\n",
    "\n",
    "\n",
    "# Load subgraphs from file, or generate them if file does not exist\n",
    "if not os.path.isfile('subgraphs.json'):\n",
    "    # Generate subgraphs based on the dataset\n",
    "    subgraphs = generate_subgraphs(graph_data, num_subgraphs=1000, min_nodes=3, max_nodes=15)\n",
    "    with open('subgraphs.json', 'w') as f:\n",
    "        json.dump(subgraphs, f)\n",
    "else:\n",
    "    with open('subgraphs.json', 'r') as f:\n",
    "        subgraphs = json.load(f)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f8cbbe1cb5e2657"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generate dataset from all subgraphs\n",
    "dataset = []\n",
    "for i in range(len(subgraphs)):\n",
    "    user_edge_index = []\n",
    "    for link in subgraphs[i]['links']:\n",
    "        source_idx = node_mapping.get(link['source'])\n",
    "        target_idx = node_mapping.get(link['target'])\n",
    "        # Add edge only if both nodes are on the subgraph\n",
    "        if source_idx is not None and target_idx is not None:\n",
    "            user_edge_index.append([source_idx, target_idx])\n",
    "    user_edge_index = torch.tensor(user_edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # Convert subgraphs nodes of the small graph\n",
    "    user_node_index = []\n",
    "    for link in subgraphs[i]['nodes']:\n",
    "        node_idx = node_mapping.get(link['id'])\n",
    "        if node_idx is not None:\n",
    "            user_node_index.append(node_idx)\n",
    "    user_node_indices = large_dataset.x[user_node_index]\n",
    "\n",
    "    # Make a mask for the subgraph nodes\n",
    "    user_mask = torch.zeros_like(large_dataset.x)\n",
    "    for idx in user_node_index:\n",
    "        user_mask[idx] = 1\n",
    "    masked_features = large_dataset.x * user_mask\n",
    "\n",
    "    # Create a dataset from the subgraph using the same features and labels as the original dataset\n",
    "    user_data = Data(x=masked_features, edge_index=user_edge_index, y=labels)\n",
    "\n",
    "    dataset.append(user_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3ad2f9acd020e7e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a model object\n",
    "# model = GCN(large_dataset.num_node_features, 64, large_dataset.num_node_features)\n",
    "# model = GAT(large_dataset.num_node_features, 64, large_dataset.num_node_features)\n",
    "model = GraphSAGE(large_dataset.num_node_features, 64, large_dataset.num_node_features)\n",
    "#model.cuda()\n",
    "model.train()\n",
    "\n",
    "# Init optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "615ae37aedf5b80f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def common_neighbors(edge_index, num_nodes):\n",
    "    # Создание списка соседей для каждого узла\n",
    "    neighbors = {i: set() for i in range(num_nodes)}\n",
    "    for edge in edge_index.t().tolist():\n",
    "        neighbors[edge[0]].add(edge[1])\n",
    "        neighbors[edge[1]].add(edge[0])\n",
    "\n",
    "    return neighbors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff563da7df179428"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_negative_samples(edge_index, num_nodes, num_neg_samples, max_attempts=1000):\n",
    "    neighbors = common_neighbors(edge_index, num_nodes)\n",
    "    negative_samples = []\n",
    "    attempts = 0\n",
    "\n",
    "    while len(negative_samples) < num_neg_samples and attempts < max_attempts:\n",
    "        node1 = random.choice(range(num_nodes))\n",
    "        node2 = random.choice(range(num_nodes))\n",
    "\n",
    "        # Проверяем, что узлы не связаны и имеют общих соседей\n",
    "        if node1 != node2 and node2 not in neighbors[node1]:\n",
    "            common_neigh = neighbors[node1].intersection(neighbors[node2])\n",
    "            # Условие можно ослабить, уменьшив требуемое количество общих соседей\n",
    "            if len(common_neigh) > 0:  # Узлы имеют общих соседей\n",
    "                negative_samples.append([node1, node2])\n",
    "\n",
    "        attempts += 1\n",
    "\n",
    "    return negative_samples"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0b03f74495a5ad8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Обновление функции обучения\n",
    "def train(model, optimizer, subgraph, positive_edges, negative_edges):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    #subgraph.cuda()\n",
    "\n",
    "    # Получаем эмбеддинги узлов\n",
    "    node_embeddings = model(subgraph.x, subgraph.edge_index)\n",
    "\n",
    "    # Подготовка меток и объединение положительных и отрицательных примеров\n",
    "    labels = torch.cat([torch.ones(len(positive_edges)), torch.zeros(len(negative_edges))], dim=0).to(subgraph.x.device)\n",
    "\n",
    "    # Убедимся, что edges имеет правильный тип данных\n",
    "    edges = torch.cat([torch.tensor(positive_edges), torch.tensor(negative_edges)], dim=0).to(subgraph.x.device).long()\n",
    "\n",
    "    # Создаём эмбеддинги рёбер\n",
    "    edge_embeddings = torch.cat([node_embeddings[edges[:, 0]], node_embeddings[edges[:, 1]]], dim=1)\n",
    "\n",
    "    # Предсказание вероятности наличия связи\n",
    "    predictions = torch.sigmoid(model.edge_predictor(edge_embeddings)).squeeze()\n",
    "\n",
    "    # Вычисление потерь и обновление параметров модели\n",
    "    loss = F.binary_cross_entropy(predictions, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29326fec38ebfbf8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Model training\n",
    "loss_values = []\n",
    "for epoch in range(2):\n",
    "    for subgraph in dataset:\n",
    "        positive_edges = subgraph.edge_index.t().tolist()\n",
    "        negative_edges = generate_negative_samples(subgraph.edge_index, subgraph.num_nodes, len(positive_edges))\n",
    "        if len(negative_edges) == 0:\n",
    "            continue\n",
    "        loss = train(model, optimizer, subgraph, positive_edges, negative_edges)\n",
    "        loss_values.append(loss)\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Save model to file\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b50a280e501ca41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_values, label='Training Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "#plt.savefig('training_loss.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b66c5ed315865af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test prediction\n",
    "user_graph_data = {\n",
    "    \"nodes\": [\n",
    "        {\"id\": \"node.js\"},\n",
    "        {\"id\": \"react.js\"},\n",
    "        {\"id\": \"javascript\"},\n",
    "        {\"id\": \"angularjs\"}\n",
    "    ],\n",
    "    \"links\": [\n",
    "        {\"source\": \"node.js\", \"target\": \"react.js\"},\n",
    "        {\"source\": \"javascript\", \"target\": \"node.js\"},\n",
    "        {\"source\": \"angularjs\", \"target\": \"javascript\"},\n",
    "        {\"source\": \"angularjs\", \"target\": \"react.js\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert subgraphs edges of the small graph\n",
    "user_edge_index = []\n",
    "for link in user_graph_data['links']:\n",
    "    source_idx = node_mapping.get(link['source'])\n",
    "    target_idx = node_mapping.get(link['target'])\n",
    "    # Add edge only if both nodes are on the subgraph\n",
    "    if source_idx is not None and target_idx is not None:\n",
    "        user_edge_index.append([source_idx, target_idx])\n",
    "\n",
    "# Convert to PyTorch Geometric format\n",
    "user_edge_index = torch.tensor(user_edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Convert subgraphs nodes of the small graph\n",
    "user_node_index = []\n",
    "for link in user_graph_data['nodes']:\n",
    "    node_idx = node_mapping.get(link['id'])\n",
    "    if node_idx is not None:\n",
    "        user_node_index.append(node_idx)\n",
    "\n",
    "# Make a mask for the subgraph nodes\n",
    "user_mask = torch.zeros_like(large_dataset.x)\n",
    "for idx in user_node_index:\n",
    "    user_mask[idx] = 1\n",
    "masked_features = large_dataset.x * user_mask\n",
    "\n",
    "# Create a dataset from the subgraph using the same features and labels as the original dataset\n",
    "user_data = Data(x=masked_features, edge_index=user_edge_index, y=large_dataset.y)\n",
    "#user_data.cuda()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdbcf4ff1f145cda"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "from redkg.visualization.contracts.graph_contract import GraphContract\n",
    "from redkg.visualization.contracts.graph_visualization_contract import GraphVisualizationContract\n",
    "from redkg.visualization.graph_visualization import GraphVisualizer\n",
    "\n",
    "# user_edge_list = list(map(tuple, user_edge_index.t().tolist()))\n",
    "# user_edge_list = [(node_index[edge[0]], node_index[edge[1]]) for edge in user_edge_list]\n",
    "\n",
    "user_node_list = [node['id'] for node in user_graph_data['nodes']]\n",
    "user_index = {id: number for number, id in enumerate(user_node_list)}\n",
    "user_edge_list = [\n",
    "    (user_index[edge['source']], user_index[edge['target']])\n",
    "    for edge in user_graph_data['links']\n",
    "]\n",
    "user_edge_weights = [1.0 for _ in range(len(user_edge_list))]\n",
    "\n",
    "graph_contract: GraphContract = GraphContract(\n",
    "    vertex_num=len(user_graph_data['nodes']),\n",
    "    edge_list=(user_edge_list, user_edge_weights),\n",
    "    edge_num=len(user_graph_data['links']),\n",
    "    edge_weights=list(tensor(user_edge_weights * 2)),\n",
    ")\n",
    "\n",
    "vis_contract: GraphVisualizationContract = GraphVisualizationContract(\n",
    "    graph=graph_contract,\n",
    "    #font_size=4.0,\n",
    "    vertex_label=user_node_list\n",
    ")\n",
    "\n",
    "vis: GraphVisualizer = GraphVisualizer(vis_contract)\n",
    "fig = vis.draw()\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4def704a2e3b8c3c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_neighbors(edge_index, node_idx):\n",
    "    neighbors = set()\n",
    "    for edge in edge_index.t().tolist():\n",
    "        if edge[0] == node_idx:\n",
    "            neighbors.add(edge[1])\n",
    "        elif edge[1] == node_idx:\n",
    "            neighbors.add(edge[0])\n",
    "    return neighbors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73302f48548b4b84"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Вычисление вероятностей связей\n",
    "def predict_edges(model, data, edge_candidates):\n",
    "    with torch.no_grad():\n",
    "        node_embeddings = model(data.x, data.edge_index)\n",
    "        probabilities = []\n",
    "\n",
    "        for edge in edge_candidates:\n",
    "            edge_features = torch.cat([node_embeddings[edge[0]], node_embeddings[edge[1]]], dim=0)\n",
    "            prob = torch.sigmoid(model.edge_predictor(edge_features.unsqueeze(0))).item()\n",
    "            probabilities.append((edge, prob))\n",
    "\n",
    "        return probabilities"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aef0e12790ccc580"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Генерация кандидатов на связи\n",
    "user_existing_edges = set(tuple(sorted((e[0].item(), e[1].item()))) for e in user_edge_index.t())\n",
    "user_node_pairs = set(tuple(sorted((node1, node2))) for node1 in user_node_index for node2 in user_node_index if node1 != node2)\n",
    "possible_large_graph_edges = set(tuple(sorted((e[0].item(), e[1].item()))) for e in large_dataset.edge_index.t())\n",
    "\n",
    "# Фильтрация possible_edges: только связи, которые возможны в большом графе и отсутствуют на графе пользователя\n",
    "possible_edges = [list(edge) for edge in possible_large_graph_edges if\n",
    "                  edge not in user_existing_edges and edge not in user_node_pairs]\n",
    "\n",
    "# Switch model to evaluation state\n",
    "model.eval()\n",
    "\n",
    "# Вычисление вероятностей связей и выбор топ-10\n",
    "edge_probabilities = predict_edges(model, user_data, possible_edges)\n",
    "edge_probabilities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Top o most possible edges\n",
    "for i, (edge, prob) in enumerate(edge_probabilities[:10]):\n",
    "    nodes = [node_index[edge[0]], node_index[edge[1]]]\n",
    "    #print(f\"| [{edge[0]}] {nodes[0]} | [{edge[1]}] {nodes[1]} | {prob} |\")\n",
    "    print(f\"Edge: [{edge[0]:3}: {nodes[0]:15}] <=> [{edge[1]:3}: {nodes[1]:15}] with probability {prob}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf82b176cdaa53b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2822477c73214a01"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
